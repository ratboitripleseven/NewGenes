{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0b8896ad70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        if not self.training:\n",
    "            print(embeds)\n",
    "            print(f'embed {embeds.dtype}')\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        #print(lstm_out.size())\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        #print(tag_space.size())\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1389, -1.2024, -0.9693],\n",
      "        [-1.1065, -1.2200, -0.9834],\n",
      "        [-1.1286, -1.2093, -0.9726],\n",
      "        [-1.1190, -1.1960, -0.9916],\n",
      "        [-1.0137, -1.2642, -1.0366]])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tag_scores size torch.Size([5, 3])\n",
      "targets torch.Size([5])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tag_scores size torch.Size([4, 3])\n",
      "targets torch.Size([4])\n",
      "tensor([1, 2, 0, 1])\n",
      "tensor([[ 7.3412e-01,  8.0676e-04, -3.3758e-01,  6.2446e-01,  2.0486e-01,\n",
      "         -1.0973e+00],\n",
      "        [-9.2287e-01, -7.1421e-01,  9.6096e-01, -1.2692e+00, -1.8903e-01,\n",
      "         -6.0069e-01],\n",
      "        [ 6.4489e-01, -6.8376e-01,  2.1375e+00,  6.7106e-01,  3.9116e-01,\n",
      "         -7.5065e-01],\n",
      "        [ 4.5504e-01,  1.3352e+00, -9.5383e-01, -1.2280e+00,  3.4942e+00,\n",
      "         -2.5017e+00],\n",
      "        [-4.8974e-01, -5.7369e-02,  9.1785e-01, -1.1828e+00,  9.0761e-01,\n",
      "          3.0940e-01]])\n",
      "embed torch.float32\n",
      "tensor([[-0.0462, -4.0106, -3.6096],\n",
      "        [-4.8205, -0.0286, -3.9045],\n",
      "        [-3.7876, -4.1355, -0.0394],\n",
      "        [-0.0185, -4.7874, -4.6013],\n",
      "        [-5.7881, -0.0186, -4.1778]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        print(f'tag_scores size {tag_scores.size()}')\n",
    "        print(f'targets {targets.size()}')\n",
    "        print(targets)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM HGT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "#from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch import autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making my dataset more like the pipeline at works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "warning.... thisis dataset cast data and length as float32! is this alright?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGTDBDatasetSequential(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_type, partition_file, partition, drop_na=False):\n",
    "        \n",
    "        if partition not in ['train','test','valid']:\n",
    "            raise ValueError('not partition or train, test or valid!')\n",
    "        \n",
    "        self.partition = partition\n",
    "        self.data_type = data_type\n",
    "        self.partition_file = partition_file\n",
    "        self.min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        # self.one_hot_encoder = preprocessing.OneHotEncoder()\n",
    "        self.drop_na = drop_na\n",
    "        self.null_count = 0\n",
    "        self.na_count = 0\n",
    "        \n",
    "        self.max_sequence = 0\n",
    "        \n",
    "        # load all in ram perhaps?\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self.data_seq_length = []\n",
    "        \n",
    "        \n",
    "        self._init_dataset()\n",
    "        \n",
    "\n",
    "    \n",
    "    def _load_single_file(self, species, data_type):\n",
    "        '''\n",
    "        replace this shit\n",
    "        some of this shit is still legacy!!!\n",
    "        '''\n",
    "\n",
    "        preprocessed_path = \"data/HGTDB/preprocessed_data\"\n",
    "        \n",
    "        csv_list = os.listdir(preprocessed_path)\n",
    "        \n",
    "    \n",
    "        if species is not None:\n",
    "            csv_file = None\n",
    "            for path in csv_list:\n",
    "                # check if current path is a file\n",
    "                if os.path.basename(path).replace(\".csv\", \"\") == species:\n",
    "                    csv_file =os.path.join(preprocessed_path, path)\n",
    "                    df = pd.read_csv(csv_file, index_col='ID')\n",
    "            if csv_file is None:\n",
    "                raise ValueError(f'{species} not found')\n",
    "        else:\n",
    "            raise ValueError(f'{species} not found')\n",
    "            \n",
    "        # set dataset according to data type\n",
    "        if data_type == 'A':\n",
    "            df = df.drop(columns=[\"FunctionCode\",\"Strand\",\"AADev\",\"Length\",\"SD1\",\"SD2\",\"SD3\",\"SDT\",\"Mah\"]) # only GC1,GC2,GC3,GCT\n",
    "\n",
    "        \n",
    "        # count nulls!\n",
    "        #df.bfill(inplace=True)\n",
    "        self.null_count +=df.isnull().sum().sum()\n",
    "        self.na_count +=df.isna().sum().sum()\n",
    "        if df.isna().sum().sum() >0:\n",
    "            print(df[df.isna().any(axis=1)])\n",
    "            \n",
    "        df = df.bfill(axis='columns')\n",
    "        #for column in df.columns:\n",
    "        #    df[column] = df[column].fillna(0)\n",
    "        self.null_count +=df.isnull().sum().sum()\n",
    "        self.na_count +=df.isna().sum().sum()\n",
    "        if df.isna().sum().sum() >0:\n",
    "            print(df[df.isna().any(axis=1)])\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        #df.dropna(inplace=True)\n",
    "        \n",
    "        #after replacing nan\n",
    "        #null_count = 0\n",
    "        #na_count = 0\n",
    "        #null_count +=df.isnull().sum().sum()\n",
    "        #na_count +=df.isna().sum().sum()\n",
    "        #print(null_count)\n",
    "        #print(na_count)\n",
    "        \n",
    "        # return as numpy array\n",
    "        # labels are not affected since there is only two options 0,1\n",
    "        df=(df-df.min())/(df.max()-df.min())\n",
    "        array = df.values\n",
    "        x = array[:,0:-1]\n",
    "        y = array[:,-1]\n",
    "        y = np.expand_dims(y, axis=1)\n",
    "        #OHE = preprocessing.OneHotEncoder()\n",
    "        #OHE.fit(y)\n",
    "        #y = OHE.transform(y).toarray()\n",
    "        return x,y \n",
    "        \n",
    "    \n",
    "    def _init_dataset(self):\n",
    "        '''\n",
    "        '''\n",
    "        partition_frame = pd.read_csv(self.partition_file)\n",
    "        partition_frame = partition_frame[partition_frame['partition']==self.partition].reset_index(drop=True)\n",
    "        \n",
    "        for i in range(len(partition_frame)):\n",
    "            x,y = self._load_single_file(partition_frame.loc[i,'file'], self.data_type)\n",
    "            \n",
    "            if self.max_sequence<len(x):\n",
    "                self.max_sequence = len(x)\n",
    "                \n",
    "            \n",
    "            self.data_x.append(torch.from_numpy(np.float32(x)))\n",
    "            self.data_y.append(torch.from_numpy(np.float32(y)))\n",
    "            self.data_seq_length.append(torch.tensor(len(x)))\n",
    "            \n",
    "        # pad 'sequences'\n",
    "        # padded stuff are tagged as 0!\n",
    "        self.data_x = pad_sequence(self.data_x, batch_first=True)\n",
    "        self.data_y = pad_sequence(self.data_y, batch_first=True)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        datum = self.data_x[ind]\n",
    "        label = self.data_y[ind]\n",
    "        seq_length = self.data_seq_length[ind]\n",
    "        \n",
    "        output = {\n",
    "            \"datum\" : datum,\n",
    "            \"seq_length\" : seq_length,\n",
    "            \"label\" : label\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         GC1  GC2  GC3  GCT  HGT\n",
      "ID                              \n",
      "SCO3176  NaN  NaN  NaN  NaN    0\n",
      "4187\n",
      "782\n",
      "2660\n"
     ]
    }
   ],
   "source": [
    "#hgtdb_train = HGTDBDatasetSequential('A','partition_file/HGTDB_firmicutes.csv', 'train')\n",
    "#print(hgtdb_train.max_sequence)\n",
    "#hgtdb_test = HGTDBDatasetSequential('A','partition_file/HGTDB_firmicutes.csv', 'test')\n",
    "#print(hgtdb_test.max_sequence)\n",
    "\n",
    "#hgtdb_train = HGTDBDatasetSequential('A','partition_file/HGTDB_ALL_trisplit.csv', 'train')\n",
    "#print(hgtdb_train.max_sequence)\n",
    "#hgtdb_valid = HGTDBDatasetSequential('A','partition_file/HGTDB_ALL_trisplit.csv', 'valid')\n",
    "#print(hgtdb_valid.max_sequence)\n",
    "#hgtdb_test = HGTDBDatasetSequential('A','partition_file/HGTDB_ALL_trisplit.csv', 'test')\n",
    "#print(hgtdb_test.max_sequence)\n",
    "\n",
    "\n",
    "hgtdb_train = HGTDBDatasetSequential('A','partition_file/HGTDB_firmicutes_trisplit.csv', 'train')\n",
    "print(hgtdb_train.max_sequence)\n",
    "hgtdb_valid = HGTDBDatasetSequential('A','partition_file/HGTDB_firmicutes_trisplit.csv', 'valid')\n",
    "print(hgtdb_valid.max_sequence)\n",
    "hgtdb_test = HGTDBDatasetSequential('A','partition_file/HGTDB_firmicutes_trisplit.csv', 'test')\n",
    "print(hgtdb_test.max_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=hgtdb_train,batch_size=2,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=hgtdb_valid,batch_size=2,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=hgtdb_test,batch_size=2,shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking if there is nan inside the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "for (idx, data) in enumerate(train_loader):\n",
    "    print(torch.isnan(data['datum']).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "for (idx, data) in enumerate(valid_loader):\n",
    "    print(torch.isnan(data['datum']).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "for (idx, data) in enumerate(test_loader):\n",
    "    print(torch.isnan(data['datum']).any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "none of my dataset has nan!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on examples -> sequence x embedding size\n",
    "\n",
    "our case -> minibatch x sequence x embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n",
      "torch.Size([2, 4187, 4])\n",
      "torch.Size([2, 4187, 1])\n"
     ]
    }
   ],
   "source": [
    "for (idx, data) in enumerate(train_loader):\n",
    "    #print(data)\n",
    "    print(data['datum'].size())\n",
    "    input = rnn.pack_padded_sequence(data['datum'], lengths=data['seq_length'], batch_first=True, enforce_sorted=False)\n",
    "    #print(input)\n",
    "    print(data['label'].size())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_scores is sigmoided... is this correct?\n",
    "\n",
    "modified as binary classifier. Inspired by https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHGTTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTMHGTTagger, self).__init__()\n",
    "        self.last_epoch= 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden2hidden=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        data, seq_length = input\n",
    "        \n",
    "        # pack padded sequence.. exp from work\n",
    "        input = rnn.pack_padded_sequence(data, lengths=seq_length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        #input to model\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        \n",
    "        # unpack\n",
    "        # apparently this unpacks them? https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        output, input_sizes = rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        # to hidden, softmax and sigmoid!\n",
    "        output = self.relu(self.hidden2hidden(output))\n",
    "        output = torch.sigmoid(self.hidden2tag(output))\n",
    "        \n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return output, input_sizes\n",
    "        #return tag_scores, input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHGTTagger_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTMHGTTagger_v2, self).__init__()\n",
    "        self.last_epoch= 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden2hidden=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        data, seq_length = input\n",
    "        \n",
    "        # pack padded sequence.. exp from work\n",
    "        input = rnn.pack_padded_sequence(data, lengths=seq_length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        #input to model\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        \n",
    "        # unpack\n",
    "        # apparently this unpacks them? https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        output, input_sizes = rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.relu(self.hidden2hidden(output))\n",
    "        output = torch.sigmoid(self.hidden2tag(output))\n",
    "        \n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return output, input_sizes\n",
    "        #return tag_scores, input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMHGTTagger_v3(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(LSTMHGTTagger_v3, self).__init__()\n",
    "        \n",
    "        self.last_epoch= 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.act_func = nn.Sigmoid()\n",
    "        self.hidden2hidden=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        data, seq_length = input\n",
    "        \n",
    "        # pack padded sequence.. exp from work\n",
    "        input = rnn.pack_padded_sequence(data, lengths=seq_length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        #input to model\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        \n",
    "        # unpack\n",
    "        # apparently this unpacks them? https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        output, input_sizes = rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.act_func(self.hidden2hidden(output))\n",
    "        output = self.act_func(self.hidden2tag(output))\n",
    "        \n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # tag_scores = F.softmax(tag_space, dim=1)\n",
    "        return output, input_sizes\n",
    "        #return tag_scores, input_sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is BCE correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMHGTTagger(4, 100, 2)\n",
    "#model = LSTMHGTTagger(4, 100, 1)\n",
    "#model = LSTMHGTTagger_v2(4,100,1)\n",
    "model = LSTMHGTTagger_v3(4,100,1)\n",
    "\n",
    "# loss_function = nn.NLLLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "#loss_function = nn.CrossEntropyLoss\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why does size of tag scores and data label have different sizes?\n",
    "datasethas nan issues!\n",
    "current solution to replace nan is not working as expected!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,dataloader):\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    \n",
    "    for (idx, data) in enumerate(dataloader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = (data['datum'], data['seq_length'])\n",
    "            output, input_sizes = model(inputs)\n",
    "            \n",
    "            targets = rnn.pack_padded_sequence(data['label'], lengths=data['seq_length'], batch_first=True, enforce_sorted=False)\n",
    "            targets,_ = rnn.pad_packed_sequence(targets, batch_first=True)\n",
    "            \n",
    "            #accuracy = (output.round() == targets).float().mean()\n",
    "            #print(accuracy)\n",
    "            \n",
    "\n",
    "        targets = torch.squeeze(targets,2)\n",
    "        # the squeeze here is needed and fine\n",
    "        output = torch.squeeze(output,2)\n",
    "        \n",
    "        # this one not sure! it takes away the batch!\n",
    "        \n",
    "\n",
    "        for i in range(targets.size(0)):\n",
    "            y_true_list = [*y_true_list, *targets[i].detach().numpy()]\n",
    "            y_pred_list = [*y_pred_list, *output[i].detach().round().numpy()]\n",
    "\n",
    "    #print(len(y_pred_list))\n",
    "    #print(len(y_true_list))\n",
    "    acc = accuracy_score(y_true_list, y_pred_list)\n",
    "    print(f'test acc: {acc}')\n",
    "    \n",
    "    #print(np.isnan(y_pred_list))\n",
    "    #try:\n",
    "    #    acc = accuracy_score(y_true_list, y_pred_list)\n",
    "    #except:\n",
    "    #    print(y_pred_list)\n",
    "    #    print(y_true_list)\n",
    "    #print(acc)\n",
    "    #report = classification_report(y_true_list, y_pred_list)      \n",
    "    #pred_ones = (preds_list.round() == 1.).float().sum()\n",
    "    #actual_ones = (targets_list == 1.).float().sum()\n",
    "    #print(f'pred hgt: {pred_ones}')\n",
    "    #print(f'actual hgt: {actual_ones}')\n",
    "    #print(f'percentage {pred_ones/actual_ones}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd.detect_anomaly\n",
    "with autograd.detect_anomaly():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,loss_function, optimizer, train_loader, valid_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.last_epoch += 1\n",
    "        print(f'training epoch:{model.last_epoch}')\n",
    "        acc_loss = 0.\n",
    "        for (idx, data) in enumerate(train_loader):\n",
    "            # print(idx)\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            \n",
    "            inputs = (data['datum'], data['seq_length'])\n",
    "            output,_ = model(inputs)\n",
    "            \n",
    "            # bad code but it works for now\n",
    "            # it basically packs padded and unpad them....\n",
    "            targets = rnn.pack_padded_sequence(data['label'], lengths=data['seq_length'], batch_first=True, enforce_sorted=False)\n",
    "            targets,_ = rnn.pad_packed_sequence(targets, batch_first=True)\n",
    "            \n",
    "            \n",
    "            #print(output.size())\n",
    "            #print(targets.size())\n",
    "            \n",
    "            \n",
    "            #loss = loss_function(output, targets)\n",
    "            #loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(),1.)\n",
    "            \n",
    "            try: \n",
    "                loss = loss_function(output, targets)\n",
    "                loss.backward()\n",
    "            except:\n",
    "                print('something broken')\n",
    "                #with autograd.detect_anomaly():\n",
    "                #    inputs = (data['datum'], data['seq_length'])\n",
    "                #    output,_ = model(inputs)\n",
    "                #    #targets = rnn.unpad_sequence(data['label'],data['seq_length'], batch_first=True)\n",
    "                #    loss = loss_function(output, targets)\n",
    "                #    loss.backward()\n",
    "                    \n",
    "                #print(output.max())\n",
    "                break\n",
    "            \n",
    "            optimizer.step()\n",
    "            acc_loss += loss.item()\n",
    "\n",
    "        test_model(model, valid_loader)\n",
    "            \n",
    "        print(f'acculumative loss: {acc_loss/len(train_loader)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch:1\n",
      "test acc: 0.03506435863293387\n",
      "acculumative loss: 0.8917982512050204\n",
      "training epoch:2\n",
      "test acc: 0.03506435863293387\n",
      "acculumative loss: 0.8889279166857401\n",
      "training epoch:3\n",
      "test acc: 0.03657407407407407\n",
      "acculumative loss: 0.8852854702207777\n",
      "training epoch:4\n",
      "test acc: 0.03627180899908173\n",
      "acculumative loss: 0.8828771776623197\n",
      "training epoch:5\n",
      "test acc: 0.03506435863293387\n",
      "acculumative loss: 0.878672911061181\n",
      "training epoch:6\n",
      "test acc: 0.03506435863293387\n",
      "acculumative loss: 0.8750120997428894\n",
      "training epoch:7\n",
      "test acc: 0.03627180899908173\n",
      "acculumative loss: 0.871995316611396\n",
      "training epoch:8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(model, loss_function, optimizer, train_loader, valid_loader, \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_function, optimizer, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m inputs \u001b[39m=\u001b[39m (data[\u001b[39m'\u001b[39m\u001b[39mdatum\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39mseq_length\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m output,_ \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     15\u001b[0m \u001b[39m# bad code but it works for now\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# it basically packs padded and unpad them....\u001b[39;00m\n\u001b[1;32m     17\u001b[0m targets \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mpack_padded_sequence(data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m], lengths\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mseq_length\u001b[39m\u001b[39m'\u001b[39m], batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/eda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mLSTMHGTTagger_v3.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mpack_padded_sequence(data, lengths\u001b[39m=\u001b[39mseq_length, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39m#input to model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39m# unpack\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# apparently this unpacks them? https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\u001b[39;00m\n\u001b[1;32m     30\u001b[0m output, input_sizes \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mpad_packed_sequence(lstm_out, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/eda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/eda/lib/python3.11/site-packages/torch/nn/modules/rnn.py:815\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    817\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    818\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, loss_function, optimizer, train_loader, valid_loader, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only firmicutes no validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_frame = pd.read_csv('partition_file/HGTDB_firmicutes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cperf</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tteng</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mgen</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mpneu</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mpul</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uure</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cglu</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mtub</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mtub2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mlep</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scoel</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bsub</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bhal</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>linno</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lmono</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sau2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sau1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sau3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llac</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spyo</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spyo2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spneu1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spneu2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>caceto</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file partition\n",
       "0    cperf      test\n",
       "1    tteng      test\n",
       "2     mgen      test\n",
       "3    mpneu      test\n",
       "4     mpul      test\n",
       "5     uure      test\n",
       "6     cglu     train\n",
       "7     mtub     train\n",
       "8    mtub2     train\n",
       "9     mlep     train\n",
       "10   scoel     train\n",
       "11    bsub     train\n",
       "12    bhal     train\n",
       "13   linno     train\n",
       "14   lmono     train\n",
       "15    sau2     train\n",
       "16    sau1     train\n",
       "17    sau3     train\n",
       "18    llac     train\n",
       "19    spyo     train\n",
       "20   spyo2     train\n",
       "21  spneu1     train\n",
       "22  spneu2     train\n",
       "23  caceto     train"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datum': tensor([[0.6340, 0.4471, 0.4516, 0.5343],\n",
       "         [0.6404, 0.4801, 0.3985, 0.5286],\n",
       "         [0.7404, 0.4107, 0.5901, 0.6314],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]], dtype=torch.float64),\n",
       " 'seq_length': 4111,\n",
       " 'label': tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgtdb_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"data/HGTDB/preprocessed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = []\n",
    "for path in os.listdir(ROOT_FOLDER):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(ROOT_FOLDER, path)):\n",
    "        downloaded_files.append(path.replace('.csv', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_eidx = int(6/8*len(downloaded_files))\n",
    "valid_eidx = int(7/8*len(downloaded_files))\n",
    "test_eidx = int(8/8*len(downloaded_files))\n",
    "\n",
    "partition_list_of_dict = []\n",
    "for i,f in enumerate(downloaded_files):\n",
    "    \n",
    "    if i < training_eidx:\n",
    "        partition = 'train'\n",
    "    elif i>= training_eidx and i<valid_eidx:\n",
    "        partition = 'valid'\n",
    "    else:\n",
    "        partition = 'test'\n",
    "    temp = {\n",
    "        \"file\" : f,\n",
    "        \"partition\": partition\n",
    "    }\n",
    "    partition_list_of_dict.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'spneu1', 'partition': 'train'},\n",
       " {'file': 'llac', 'partition': 'train'},\n",
       " {'file': 'ypestis', 'partition': 'train'},\n",
       " {'file': 'fnucl', 'partition': 'train'},\n",
       " {'file': 'bbur', 'partition': 'train'},\n",
       " {'file': 'sent', 'partition': 'train'},\n",
       " {'file': 'sau2', 'partition': 'train'},\n",
       " {'file': 'cjen', 'partition': 'train'},\n",
       " {'file': 'bsub', 'partition': 'train'},\n",
       " {'file': 'atum2c1', 'partition': 'train'},\n",
       " {'file': 'ecoli3', 'partition': 'train'},\n",
       " {'file': 'vvul2c2', 'partition': 'train'},\n",
       " {'file': 'bhal', 'partition': 'train'},\n",
       " {'file': 'xcamp', 'partition': 'train'},\n",
       " {'file': 'vcolc2', 'partition': 'train'},\n",
       " {'file': 'cpneu', 'partition': 'train'},\n",
       " {'file': 'xcitri', 'partition': 'train'},\n",
       " {'file': 'sau3', 'partition': 'train'},\n",
       " {'file': 'vvul1c2', 'partition': 'train'},\n",
       " {'file': 'pmul', 'partition': 'train'},\n",
       " {'file': 'synecho', 'partition': 'train'},\n",
       " {'file': 'rconorii', 'partition': 'train'},\n",
       " {'file': 'bmelic1', 'partition': 'train'},\n",
       " {'file': 'dra1', 'partition': 'train'},\n",
       " {'file': 'mpul', 'partition': 'train'},\n",
       " {'file': 'cglu', 'partition': 'train'},\n",
       " {'file': 'rsolac2', 'partition': 'train'},\n",
       " {'file': 'ctep', 'partition': 'train'},\n",
       " {'file': 'nmen1', 'partition': 'train'},\n",
       " {'file': 'cperf', 'partition': 'train'},\n",
       " {'file': 'mtub2', 'partition': 'train'},\n",
       " {'file': 'uure', 'partition': 'train'},\n",
       " {'file': 'atum1c1', 'partition': 'train'},\n",
       " {'file': 'ecoli2', 'partition': 'train'},\n",
       " {'file': 'ctra', 'partition': 'train'},\n",
       " {'file': 'mlep', 'partition': 'train'},\n",
       " {'file': 'mloti', 'partition': 'train'},\n",
       " {'file': 'hpyl99', 'partition': 'train'},\n",
       " {'file': 'rsola', 'partition': 'train'},\n",
       " {'file': 'xfas', 'partition': 'train'},\n",
       " {'file': 'nmen2', 'partition': 'train'},\n",
       " {'file': 'atum1c2', 'partition': 'train'},\n",
       " {'file': 'vvul1c1', 'partition': 'train'},\n",
       " {'file': 'btheta', 'partition': 'train'},\n",
       " {'file': 'ccres', 'partition': 'train'},\n",
       " {'file': 'tteng', 'partition': 'train'},\n",
       " {'file': 'spyo2', 'partition': 'train'},\n",
       " {'file': 'styp', 'partition': 'train'},\n",
       " {'file': 'paer', 'partition': 'train'},\n",
       " {'file': 'linno', 'partition': 'train'},\n",
       " {'file': 'nsp', 'partition': 'train'},\n",
       " {'file': 'bmelic2', 'partition': 'train'},\n",
       " {'file': 'cpneu2', 'partition': 'train'},\n",
       " {'file': 'dra2', 'partition': 'train'},\n",
       " {'file': 'bsp', 'partition': 'train'},\n",
       " {'file': 'baphi', 'partition': 'train'},\n",
       " {'file': 'cmur', 'partition': 'train'},\n",
       " {'file': 'hinf', 'partition': 'train'},\n",
       " {'file': 'mgen', 'partition': 'valid'},\n",
       " {'file': 'vvul2c1', 'partition': 'valid'},\n",
       " {'file': 'rpxx', 'partition': 'valid'},\n",
       " {'file': 'ecoli', 'partition': 'valid'},\n",
       " {'file': 'vcolc1', 'partition': 'valid'},\n",
       " {'file': 'mtub', 'partition': 'valid'},\n",
       " {'file': 'spyo', 'partition': 'valid'},\n",
       " {'file': 'lmono', 'partition': 'valid'},\n",
       " {'file': 'smel', 'partition': 'valid'},\n",
       " {'file': 'scoel', 'partition': 'valid'},\n",
       " {'file': 'tmar', 'partition': 'test'},\n",
       " {'file': 'spneu2', 'partition': 'test'},\n",
       " {'file': 'aquae', 'partition': 'test'},\n",
       " {'file': 'cpneu3', 'partition': 'test'},\n",
       " {'file': 'tpal', 'partition': 'test'},\n",
       " {'file': 'mpneu', 'partition': 'test'},\n",
       " {'file': 'hpyl', 'partition': 'test'},\n",
       " {'file': 'caceto', 'partition': 'test'},\n",
       " {'file': 'atum2c2', 'partition': 'test'},\n",
       " {'file': 'sau1', 'partition': 'test'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(partition_list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('HGTDB_ALL_trisplit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39m10\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer modulo by zero"
     ]
    }
   ],
   "source": [
    "10%0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
